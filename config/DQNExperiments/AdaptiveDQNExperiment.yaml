kp: [1, 1, 1, 1, 1, 1]
kd: [0, 0.2, 0.3]
ki: [0, 0, 0, 0, 0, 0]
alpha: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05]
beta: [0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
gamma: 0.99

optimizer: [Adam, Adam, Adam, Adam, Adam, Adam]
replay_memory_size: [1000000, 1000000, 1000000, 1000000, 1000000, 1000000]
batch_size: [128, 128, 128, 128, 128, 128]
learning_rate: [1e-3, 1e-3, 1e-3, 1e-3, 1e-3, 1e-3]
epsilon: [1, 1, 1, 1, 1, 1]
epsilon_decay: [0.999, 0.999, 0.999, 0.999, 0.999, 0.999]
epsilon_min: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05]
epsilon_decay_step: [1, 1, 1, 1, 1, 1]
train_step: [1, 1, 1, 1, 1, 1]
tau: [0.005, 0.005, 0.005, 0.005, 0.005, 0.005]
D_tau: [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]
inner_size: [64, 64, 64, 64, 64, 64]
meta_lr: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1]

num_episodes: 400
num_iterations: 100000
num_average: 10  # The number of steps to average out when visualizing the results
norm: 1
normalize: False
num_runs: 5

follow_trajectory: False

env: CartPole-v1 # Possible Environments include: CartPole-v1, MountainCar-v0, Acrobot-v1, LunarLander-v2, LunarLanderContinuous-v2, Pendulum-v0, MountainCarContinuous-v0, BipedalWalker-v3, BipedalWalkerHardcore-v3, CarRacing-v0, Pong-v0, MsPacman-v0, SpaceInvaders-v0, Breakout-v0, MountainCar-v0, Acrobot-v1, LunarLander-v2, LunarLanderContinuous-v2, Pendulum-v0, MountainCarContinuous-v0, BipedalWalker-v3, BipedalWalkerHardcore-v3, CarRacing-v0, Pong-v0, MsPacman-v0, SpaceInvaders-v0, Breakout-v0
use_episodes: True

debug_num_steps: 10  # The number of episodes before reward is logged to the console

seed: -1
hydra:
  run:
    dir: ${oc.env:OUTPUT_DIR,./outputs}/${hydra.job.config_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job:
    chdir: True