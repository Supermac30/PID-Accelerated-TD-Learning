kp: [1]
kd: [0, 0, 0, 0, 0, 0]
ki: [0, 0, 0.2, 0.3, 0.4, 0.5]
alpha: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05]
beta: [0.95, 0.95, 0.95, 0.95, 0.95, 0.95]
gamma: 0.99

optimizer: [Adam, Adam, Adam, Adam, Adam, Adam]
replay_memory_size: [100000, 1000000, 1000000, 1000000, 1000000, 1000000]
batch_size: [64, 128, 128, 128, 128, 128]
learning_rate: [0.0023, 1e-4, 1e-4, 1e-4, 1e-4, 1e-4]
initial_eps: [1, 1, 1, 1, 1, 1]
exploration_fraction: [0.16, 0.1, 0.1, 0.1, 0.1, 0.1]
minimum_eps: [0.04, 0.05, 0.05, 0.05, 0.05, 0.05]
gradient_steps: [128, 5, 5, 5, 5, 5]
train_freq: [256, 256, 256, 256, 256, 256]
target_update_interval: [10, 1, 1, 1, 1, 1]
tau: [1, 0.01, 0.01, 0.01, 0.01, 0.01]
d_tau: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
inner_size: [256, 64, 64, 64, 64, 64]
learning_starts: [1000, 1000, 1000, 1000, 1000, 1000]

# Gain adaptation parameters
adapt_gains: [False, True]
epsilon: [1e-5, 1e-5, 1e-5, 1e-5, 1e-5, 1e-5]
meta_lr: [1e-2, 1e-2, 1e-2, 1e-3, 5e-3, 5e-3]

slow_motion: 1

num_iterations: 50000
num_average: 10  # The number of steps to average out when visualizing the results
norm: 1
normalize: False
num_runs: 1

follow_trajectory: False

env: CartPole-v1 # Possible Environments include: CartPole-v1, MountainCar-v0, Acrobot-v1, LunarLander-v2, LunarLanderContinuous-v2, Pendulum-v0, MountainCarContinuous-v0, BipedalWalker-v3, BipedalWalkerHardcore-v3, CarRacing-v0, Pong-v0, MsPacman-v0, SpaceInvaders-v0, Breakout-v0, MountainCar-v0, Acrobot-v1, LunarLander-v2, LunarLanderContinuous-v2, Pendulum-v0, MountainCarContinuous-v0, BipedalWalker-v3, BipedalWalkerHardcore-v3, CarRacing-v0, Pong-v0, MsPacman-v0, SpaceInvaders-v0, Breakout-v0
use_episodes: True

log_interval: 1  # The number of episodes before reward is logged to the console
progress_bar: True
tensorboard_log: tensorboard


seed: -1
hydra:
  run:
    dir: ${oc.env:OUTPUT_DIR,./outputs}/${hydra.job.config_name}/${env}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job:
    chdir: True