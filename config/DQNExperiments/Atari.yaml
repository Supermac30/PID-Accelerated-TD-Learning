kp: 1
kd: 0
ki: 0
alpha: 0.05
beta: 0.95
gamma: 0.99

# Gain adaptation parameters
adapt_gains: False
epsilon: 1e-1
meta_lr: 1e-2

slow_motion: 1

num_iterations: 10000000
num_average: 10  # The number of steps to average out when visualizing the results
norm: 1
normalize: False
num_runs: 1

use_episodes: True

log_interval: 10  # The number of episodes before reward is logged
progress_bar: True
tensorboard_log: tensorboard

optimizer: Adam
replay_memory_size: 100000
batch_size: 32
learning_rate: 1e-4
initial_eps: 1
exploration_fraction: 0.1
minimum_eps: 0.01
gradient_steps: 1
train_freq: 4
target_update_interval: 1000
tau: 1
d_tau: 1
inner_size: 256
learning_starts: 100000

seed: -1


# Atari Configs
optimize_memory_usage: False

env: Pong-v0 # Possible Environments include: Pong-v0, MsPacman-v0, SpaceInvaders-v0, Breakout-v0
hydra:
  run:
    dir: ${oc.env:OUTPUT_DIR,./outputs}/DQNExperiment/${env}/${now:%Y-%m-%d}/${now:%H-%M-%S}

  job:
    chdir: True

  sweep:
    dir: outputs/DQNExperiment/${env}/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ""
  mode: MULTIRUN
  sweeper:
    params:
      adapt_gains: False, True
      meta_lr: 1e-3
      epsilon: 1
      d_tau: 1e-3