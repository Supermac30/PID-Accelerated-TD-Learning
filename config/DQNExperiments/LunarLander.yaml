kp: 1
kd: 0
ki: 0
alpha: 0.05
beta: 0.95
gamma: 0.99

# Gain adaptation parameters
adapt_gains: False
epsilon: 1e-1
meta_lr: 1e-2

slow_motion: 1

num_iterations: 100000
num_average: 10  # The number of steps to average out when visualizing the results
norm: 1
normalize: False
num_runs: 1

env: LunarLander-v2 # Possible Environments include: CartPole-v1, MountainCar-v0, Acrobot-v1, LunarLander-v2, LunarLanderContinuous-v2, Pendulum-v0, MountainCarContinuous-v0, BipedalWalker-v3, BipedalWalkerHardcore-v3, CarRacing-v0, Pong-v0, MsPacman-v0, SpaceInvaders-v0, Breakout-v0, MountainCar-v0, Acrobot-v1, LunarLander-v2, LunarLanderContinuous-v2, Pendulum-v0, MountainCarContinuous-v0, BipedalWalker-v3, BipedalWalkerHardcore-v3, CarRacing-v0, Pong-v0, MsPacman-v0, SpaceInvaders-v0, Breakout-v0
use_episodes: True

log_interval: 10  # The number of episodes before reward is logged to the console
progress_bar: True
tensorboard_log: tensorboard


optimizer: Adam
replay_memory_size: 50000
batch_size: 128
learning_rate: 6.3e-4
initial_eps: 1
exploration_fraction: 0.12
minimum_eps: 0.1
gradient_steps: -1
train_freq: 4
target_update_interval: 250
tau: 1
d_tau: 0.75
inner_size: 256
learning_starts: 0


seed: -1
hydra:
  run:
    dir: ${oc.env:OUTPUT_DIR,./outputs}/DQNExperiment/${env}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job:
    chdir: True
  mode: MULTIRUN
  sweep:
    dir: outputs/DQNExperiment/${env}/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ""

  sweeper:
    params:
      adapt_gains: True, False
      d_tau: 0.1
      epsilon: 1
      meta_lr: 0.001