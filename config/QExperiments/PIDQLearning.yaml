kp: [1, 1, 1, 1, 1]
kd: [0, 0.4]
ki: [0, 0.75, 0.2, -0.1, -0.1]
alpha: [0.05, 0.05, 0.05, 0.05, 0.05]
beta: [0.95, 0.95, 0.95, 0.95, 0.95]
<<<<<<< HEAD
decay: [0.99999, 0.99999, 0.99999, 0.99999, 0.99999]
=======
decay: [0.99999, 0.99999, 0.9999, 0.9999, 0.9999]
>>>>>>> cbd14f7687c1c43fc32541a8522c0144597b97fe
gamma: 0.99

agent_name: [Q learning, Q learning, Q learning, Q learning, Q learning]

env: chain walk
num_iterations: 30000
norm: fro
<<<<<<< HEAD
get_optimal: False # Get the optimal parameters from the database
compute_optimal: False # Run a grid search. When recompute_optimal is false, we don't run a grid search if the optimal parameters are in the database already
recompute_optimal: False  # If compute_optimal is True, run a grid search even if the optimal parameters are in the database
=======
get_optimal: True # Get the optimal parameters from the database
compute_optimal: True # Run a grid search. When recompute_optimal is false, we don't run a grid search if the optimal parameters are in the database already
recompute_optimal: True  # If compute_optimal is True, run a grid search even if the optimal parameters are in the database
>>>>>>> cbd14f7687c1c43fc32541a8522c0144597b97fe
num_repeats: 1

follow_trajectory: False

seed: 848384
hydra:
  run:
    dir: ${oc.env:OUTPUT_DIR,./outputs}/${hydra.job.config_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job:
    chdir: True